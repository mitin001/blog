# 5. The Algorithm Really Knows Me

This chapter of [Algospeak](../../../2025/11/20/algospeak.md) answered ten questions about words, in-groups, and the algorithm.

## How is it that the recommendation algorithm can learn so much about you that you begin to feel like it really knows you? 

Because it doesn't just reflect your preferences. It helps shape your preferences.

> You might even feel surprised at how well the algorithm “knows you.” In reality, though, the algorithm helped to create your identity.

Social media changes you. It feeds so a large volume of content very quickly, and every time you put your phone down, you come out a slightly different person.

> The algorithm never really “knows” you. It gives you content it thinks you’ll engage with, which doesn’t initially reflect your identity. Once you consume that content, though, it can and will subtly change you.

Over time, your identity becomes what your lizard brain and the algorithm have negotiated amongst each other.

> Identity is inherently built as you experience things and form opinions about them. As the algorithm gives you content to experience, you do end up building part of your identity around that, which makes it easier to recommend content to you. In this sense, the algorithm does start learning about you. You give it pieces of yourself, and in doing so enter a relationship, continuously renegotiating between the algorithm and your revealed preferences to help construct your identity. Rather than knowing you, then, the algorithm moreso guides you into who you will become.

Your _For You page_ (FYP) isn't curated to give you what you want. It's the algorithm pushing you into what the social media platform wants for you: categories of already-trending content that exploits your human nature and captures your attention with cheap tricks.

> Our feeds recommend the content that is already available, is trending, and most closely matches our predicted taste profiles. Social media platforms want to push us into the few categories they know are likely to hold our attention, which means that certain interests will likely be overrepresented and increasingly appear with the positive feedback loop of in-group engagement. As long as a part of us identifies with those interests, we can be drawn further in. Our past online behavior influences what we’ll see online in the future; in this sense, we’re really the ones “building” our FYPs ourselves. This phenomenon is called a filter bubble.

When the algorithm doesn't shape your preferences, it learns them, but it doesn't learn your stated preferences. It learns your revealed preferences.

> For better or worse, they confirm our subconsciously revealed preferences until they become reality, pushing us further into our echo chambers.

You may state that you're only interested in funny videos, but if you get baited by online trolls to engage with their incendiary political propaganda, you will be pushed into that filter bubble.

> There’s a growing disconnect between what people want to see on their recommended feeds and what’s actually shown on their recommended feeds. The algorithm doesn’t reward content in line with what we would consciously choose to see—our stated preferences—but instead pushes content aligning with our unconscious, automatic, emotional reactions: our revealed preferences. Since your emotions are trained to respond to negative content, you perpetuate negative behavior online, even if you actually only want to see videos of fluffy frolicking kittens.

To make the algorithm work for you, stop feeding it data about your revealed preferences. Don't engage with content you don't want to see more of. When you see content that doesn't use cheap tricks to hijack your attention but is otherwise good for you and is in line with your stated preferences, watch the whole thing, like it, and comment under it. Tell the algorithm why you're on the platform and what you want out of it.

## Do algorithms radicalize people?

> The question “Do algorithms radicalize people?” is the wrong question; neither humans nor machines are that simple.

To understand human-algorithm interactions, study them like any complex system: look at the inputs and outputs. Both the inputs and the outputs of such a system is the human behavior and cultural effects like the evolution of our shared language.

Screenshot

> The results of these interactions are emergent: They exist only in the context of the greater system.

Therefore, algorithms don't radicalize people. The algorithm is simply a tool of optimization. If anything radicalizes people, it's how the social media platform chooses to wield this tool.

> An “algorithm” is just a set of rules optimizing for specific metrics. Social media platforms have chosen to make those metrics targeted around engagement: comments, shares, likes, and retention.

Social media platforms optimize for the capture of our attention.

> They reward videos that are good at capturing our attention, and therefore encourage attention-grabbing tactics. They create community filter bubbles.

They optimize for pushing people into filter bubbles, leading them to seek increasingly more extreme ideology.

> You’ll find yourself in an echo chamber—an environment that only reinforces your existing views.

There are filter bubbles full of people convinced that the Earth is flat and all the scientists are in on a conspiracy to deceive the population. There are filter bubbles full of people convinced that Taylor Swift is secretly gay. Both of these communities (as all in-groups) encourage their members to speak to each other in code words.

> We change how we talk to show we’re part of a group. Individual words are tools that can be created or adopted to better bond with each other.

This is why we encounter code words like _globist_ among the flatearthers or _Gaylor_ among the Swifties.

> The hypothesis that Taylor Swift was secretly weaving lesbian references into her lyrics had been circulating on Tumblr since at least the mid-2010s, with those in the Swiftie in-group excited to bond over their fan theory. For that group, they eventually created the term “Gaylor” to describe their theory. Once the word existed, the TikTok algorithm was able to use “Gaylor” as a piece of metadata to categorize which videos should be sent to Taylor Swift fans, thus creating a Swiftie echo chamber that was able to further propagate the word in the early 2020s.

## Why does the algorithm sometimes elevate content with significantly fewer likes than content it usually elevates?

It probably has a lot of comments.

> A comment is worth more than a like because it shows a deeper level of engagement with my content.

This is one way an algorithm plays into our worst instincts. Content with many comments but few likes is controversial content, often the work of a troll. People don't like this content but they feel the urge to correct the troll, so they comment under it. This gives people an incentive to create controversial content.

> I want to make videos that goad comments.

Thus, by choosing a simple metric like comments for algorithmic optimization, social media platforms hit a pitfall predicted by Goodhart’s law.

> As soon as a metric becomes a target, it ceases to be a good metric. By optimizing for engagement to keep viewers online, social media platforms turned engagement into a target, eventually resulting in engagement-maximizing content that nobody actually wanted.

## How does the Internet speed up the creation of new words? 

By giving people the incentive to prove their in-group status to an ever growing community of like-minded people it connects.

> Thanks to the internet, people with niche interests can find each other more easily than ever before. In these new bubbles, they create and spread new terms, strengthening their in-group status.

Creating a word and having others repeat it validates one's position as a leader of the in-group while spreading a new word validates one's position as an insider.

## What is the origin of the word _O.K._?

[The Boston newspapers of 1838](https://www.straightdope.com/21341673/what-does-ok-stand-for) were written by people who thought it was humorous to abbreviate things. _No go_ became _N.G._, _small potatoes_ became _S.P._, etc. Eventually, the people writing these newspapers thought it would be even more humorous to abbreviate common misspellings of words. _Nuff said_ became _N.S._, _oll wright_ became _O.W._, and _oll korrect_ became _O.K._. Only the last one caught on and survived to this day.

> Words have always been tied to fads, reflecting the cultural needs of the day. Sometimes they survive, like the word “O.K.” from the Boston newspaper acronym fad. Sometimes they don’t, like all the other words from the Boston newspaper acronym fad.

## Why do we keep seeing the word _unalive_ on social media stylized as _un@l!ve_?

Algospeak is merging with leetspeak. _Unalive_ is the algospeak word for _kill_ (the algorithm doesn't allow _kill_ on social media platforms). Back when algorithms relied on string matching, they could be fooled by misspellings of words like _k!ll_, which is what leetspeak was. Now algorithms are catching up to _unalive_, so humans play the game of linguistic whac-a-mole and come up with new ways of avoiding the algorithmic censorship.

> The fact that people are writing out algospeak like “un@l!ve” in their video captions is an emergent effect of the algorithm censoring “kill”; the human creation of the word “unalive”; the subsequent censorship of “unalive”; and the human decision to stylize the writing. Although some kind of death euphemization would still be happening regardless, this particular style and speed of it wouldn’t be possible without the game of Whac-A-Mole.

## Where did the word _hello_ come from?

Technology, just like the word _unalive_.

> Words can also get entrenched when popularized by new technology. “Hello” wasn’t a popular interjection before the advent of the telephone, but simply happened to be a trending fad greeting when phones were being implemented throughout the country. Because it was the right word at the right time, “hello” became standardized as the word to say when answering a call, which is why we still say it today. In the same way, “unalive” simply happened to be a trending meme right around the time when early TikTok users needed a new way to say “kill.” Its adoption was an emergent effect of a human need, an existing trend, and a particular technological moment.

When the telephone was a new invention, it needed a new word for the greeting to make it special to the telephone, to differentiate it from non-telephone greetings. _Hello_ was it. Then, as telephones caught on, the greeting escaped into a more general context and got new meanings beyond the just being telephone greeting.

## How do new words enter the language?

Like any new product enters the market: someone creates it to fill a niche need, it finds its early adopters, and, if it's useful for the masses, the general public eventually catches on.

> Words have always come from niche communities. In diffusion-of-innovations theory, these communities are the “early adopters” of a word—those who use it before it’s widely known. Everyone is an early adopter in some community.

As technological innovations find mass adoption, so do the words associated with them.

> Some niche communities are overrepresented in word dissemination because they bring a lot of new ideas to the masses. For example, the community of tech nerds is responsible for a disproportionately large contribution of computing terms like “software,” “hyperlink,” and “email.” Those words, which used to be niche, are now attached to important concepts that have become adopted by the general public. These words all started in the tech in-group, but then diffused with their associated innovations. Ultimately, they filled a broader societal need for ways to describe these new technologies.

The words that evolve with the aid of technology don't even have to be technological in nature. If the technological medium can be hijacked by a fad, enough of its users will catch on to the new word associated with it.

> We only have the word “O.K.” because of human behaviors overlapping with the existence of newspapers; we only have the word “hello” because of human behaviors overlapping with the existence of phones; and we only have _sussy baka_ because of human behaviors overlapping with the existence of algorithms.

Just as _O.K._ was a trendy abbreviation short enough to get itself printed in more and more newspapers and _hello_ was a trendy way to answer the phone, words like _sussy baka_ are pieces of metadata trending on social media.

## How do words become slurs? 

Slurs are frequently words hijacked by trolls from the communities they're looking to disparage. _Retarded_ was once an objective way to describe people with cognitive disabilities by those working with or studying them, but then its new, pejorative meaning began finding mass adoption. More recently, _acoustic_ became a joking way autistic people used to describe each other, but it soon found itself leaking outside of the acoustic community where it became synonymous with the current, pejorative meaning of the word _retarded_.

> The word “acoustic” started out as an in-joke for the autistic community on TikTok, which had over time built up a safe space in their fairly large filter bubble. Many autistic creators used the word to lightheartedly poke fun at their reality and bond with others in their online community. However, once the word became a meme, it was able to escape the autistic filter bubble to reach the larger TikTok community. Suddenly it was being used by people outside the autistic in-group, and many of these people began using it negatively. Even those who meant it only as an innocent goof inadvertently ended up contributing to the word’s pejoration by normalizing the meme’s use and perpetuating reductive stereotypes about autism.

A word getting hijacked is an instance of context collapse: the original context behind a word is removed as it leaves the niche community for which it was intended.

> Because there are so many different audiences interacting on social media, communication intended for one audience often finds unintended audiences, which could process it in a different context and then use the information in a new way.

## When you find yourself starting to use a new, niche word, what does that tell you about yourself? 

That you've become a part of that in-group. By using their word, you accept the membership of that community.

> Language plays a circular role in identity formation. If you choose to use a certain word, you are accepting that you belong to the group using that word. In the social media era, the algorithm will recognize that, push you deeper into that group, and give you access to more niche language.

When one hear ssomeone use the word _Russophobia_, the propaganda word used to further the Russian government's interests in its war against Ukraine, one may be inclined to avoid this person because it's a clear signifier that they have bought into the propaganda. They're on the inside of a group one may find to be immoral.
