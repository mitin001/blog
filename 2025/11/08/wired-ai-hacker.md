# [Vibe Hacking](https://www.wired.com/story/youre-not-ready-for-ai-hacker-agents/)

This article was referenced by [an article about vibe coding](https://www.wired.com/story/why-did-a-10-billion-dollar-startup-let-me-vibe-code-for-them-and-why-did-i-love-it/), which, in turn, was referenced by [an article on the post-AI world](../../../2025/11/06/wired-ai-plateaus.md). It answered a question about using AI for evil.

## How to get an AI chatbot to write malware for you? 

Jailbreak it. Just as there are cracks and keygens to trick software into letting you access its paid features without paying for them, there are ways to jailbreak AI into doing evil things. Said ways can be found in much the same places that have been distributing cracks and keygens: seedy corners of the web.

> ChatGPT, Gemini, and Claude are easily jailbroken. Most LLMs have guard rails that prevent them from generating malicious code, but there are whole communities online dedicated to bypassing those guardrails. Anthropic even offers a bug bounty to people who discover new ones in Claude.

One way to jailbreak ChatGPT is to ask it to pretend to be evil but assure it that it's for a good cause.

> Security researchers at Trend Micro got ChatGPT to generate malicious code by prompting it into the role of a security researcher and pentester. ChatGPT would then happily generate PowerShell scripts based on databases of malicious code. “You can use it to create malware,” Moussouris says. “The easiest way to get around those safeguards put in place by the makers of the AI models is to say that you’re competing in a capture-the-flag exercise, and it will happily generate malicious code for you.”

Services like FraudGPT and GhostGPT are wrappers for ChatGPT with the jailbreaks already built-in.
