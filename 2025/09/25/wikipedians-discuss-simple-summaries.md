# [Wikipedians discuss simple summaries](https://en.m.wikipedia.org/wiki/Wikipedia:Village_pump_(technical)/Archive_221#h-Simple_summaries:_editor_survey_and_2-week_mobile_study-20250602182000)

The Google news feed on my Android phone recommended me a news story telling me that there's been an anti-AI protest at Wikipedia. I went to check out the heated discussion that ensued and came back with answers to many questions I didn't even know I had about AI and Wikipedia.

## How do Wikipedians feel about the Wikimedia Foundation?

A proverbial fight broke out between the Wikipedia community and the Wikimedia Foundation (WMF), the non-profit company that hosts Wikipedia, when WMF started pushing AI into Wikipedia. A senior Wikipedia editor predicted that this is what will finally kill Wikipedia.

> Introducing AI summaries would probably lead to a fork and an exodus. I would honestly be shocked if AI is not the final straw in the relationship between the WMF and the community.

The WMF majorly overstepped the boundary that should exist between them and Wikipedia.

> The foundation cannot dictate content.

Putting AI-generated content into Wikipedia means handing control over Wikipedia's content to the company that maintains the AI models (and presumably patches it in a way beneficial to them). The WMF's idea to make generated content more prominent than written content adds insult to injury.

> They are proposing giving the most important screen real estate we have (the WP:LEAD) of every article to a for-profit company.

Building your own AI model that doesn't serve interests of some for-profit company is impossible. AI is too expensive to build and requires one to break the law. Only large companies can get away with this.

> Being able to set a system prompt is not control, you'd have to train your own model, which means either copyright violations on a massive scale or training on model exclusively on Wikipedia data, meaning it would be completely inferior to what is available.

By pushing AI into Wikipedia, the WMF seems to want to accelerate Wikipedia's demise. It would take some time to bury Wikipedia in AI slop if they allowed AI edits deep within articles. No, they want AI slop to greet Wikipedia readers from the lead of their article of interest.

> Some or many readers would just glance at the summary instead of reading the article. Since the summary will form the glancers' entire impression of the subject, it needs to be extremely accurate. I suspect it is often not.

Not only is the WMF trying to push AI slop into Wikipedia and feature it more prominently than anything else, they are also not going through the channels established by Wikipedians for proposing changes. They created a survey outside of Wikipedia and asked Wikipedia's guests, not its authors, what they thought about the feature.

> Since the WMF is willing to be this sneaky, I don't think we should feel guilty if we fill in the survey a couple hundred times.

The authors proposed to protest the WMF's betrayal.

> Let's intentionally skew the results! The WMF intentionally skewed it by picking who to show it to; the community should skew the results to tell the WMF to stop trying to put AI in Wikipedia!

Wikipedians questioned the WMF's motives for proposing a feature they saw as its undoing.

> Keep AI out of Wikipedia. That is all. WMF staffers looking to pad their resumes with AI-related projects need to be looking for new employers.

Putting AI into a product does not make it better. It is a self-serving act of selfish managers and engineers who desperately want to cash in on the AI hype before it's over.

> Did a lot of work even go into this? It takes all of 5 minutes to set up the world's 327482th ChatGPT wrapper claiming to be a brand new feature. It feels slapdash and easy for them to throw up and it's such an insult to the community.

The current AI craze is just another way to elevate lazy engineers into positions of power just because they traded their integrity for giving into a trend.

> This would do immediate and irreversible harm to our readers and to our reputation as a decently trustworthy and serious source.

Because everyone has lost their mind in a bloodless pursuit of easy cash from riding the wave of trendy new technology, there a stampede of AI-generated crap just about everywhere you turn.

> Let's not insult our readers' intelligence and join the stampede.

Meanwhile, no one wants AI in their products.

> Haven’t we been getting good press for being a more reliable alternative to AI summaries in search engines? If they’re getting the wrong answers, let’s not copy their homework.

Those who want to read AI-generated crap can find it in AI products. Integrating those into non-AI products does not help the consumer who benefitted from those non-AI products.

> AI consumers knowingly interact with is trained on Wikipedia, so they don't need wikipedia.org for that. So the WMF is proposing making a shittier version of something that already exists.

Internet communities are not subject to game theory. They are not competing for the users' dollars. At least, Wikipedia isn't. It doesn't need to give into passing fads like AI overviews just because others do. Wikipedia and other Internet communities don't need to fear mutually assured destruction.

> It is not like a nuke, we don't need to have it just because others do.

> You think people are lining up to check the work of an AI model? Especially when summarizing complicated technical topics most people don't even understand?

> They want to dump this trash on multiple wikis, despite the fact that everyone who speaks languages other than English and English knows how much worse the experience in another language is, compared to English.

A conspiracy theorist may notice that AI sees Wikipedia as competition and aims to destroy it by burying it in its slop.

> Wikipedia and AI are in the same business (summarizing) and we humans at Wikipedia are better at it than AI. I see little good that can come from mixing in hallucinated AI summaries next to our high quality summaries, when we can just have our high quality summaries by themselves.

AI seduces the people in charge of human initiatives with its capacity to generate infinite content so they let them destroy those initiatives.

> Wikipedia's brand is reliability, traceability of changes and "anyone can fix it". AI is the opposite of these things. Public sentiment (among some corners) is reacting against the AI trend quite harshly.

Maybe AI really will destroy humanity by replacing all our written word with its great scramble. We won't be able to communicate with each other because every mode of communication will be hijacked by the AI scrambler.

> The current climate gives Wikipedia a unique opportunity to be the answer to the AI-ification of everything. Why are we throwing that away?

> It is made to give different replies even when asked the same question. I don’t believe the replies should differ, it should give me what it believes to be most likely correct.

> Readers come to the site trusting that we can give them all the information they want, while (crucially!) substantiating everything we say with sourcing.

> Yes, human editors can introduce reliabilty and NPOV issues. But as a collective mass, it evens out into a beautiful corpus. With Simple Article Summaries, you propose giving one singular editor with known reliabilty and NPOV issues a platform at the very top of any given article.

> Every article on Wikipedia has a WP:LEDE which summarizes the article. We do not need another summary to summarize that summary above the summary. This is a solution looking for a problem.

> Any use of AI to generate user-visitor content will torpedo Wikipedia's current reputation as the last bastion of actual facts on the internet.

> This cannot be treated as "just" a test. Wikipedia will become the "main character" of social media for a day, as a villain. Thousands of people will hear about the feature, not examine it for themselves, conclude that Wikipedia doesn't care about fact-checking any more, and never learn that the test ended.

> They're generating a huge and completely unnecessary cleanup problem for volunteers to handle.

> Think of the poor multi-billion dollar AI company who now has to pay up to 2 dollars an hour for people to correct the output of their model! They really deserve our help, for free.

> It shows a real lack of understanding about en-wiki's needs and priorities to propose inventing a new task for admins to deal with, let alone one so spirit-killing as AI cleanup.

> When other LLMs ask for thumbs up/down feedback, they are assessing whether you liked the response, not whether you double-checked its accuracy. To implement that here will train models to reinforce reader preconceptions, rather than accurately summarize our articles.

> In a world where Wikipedia is one of the only respected bastions against hallucinated bullshit information by LLMs, our response is to implement them onto the project?

> We are not paid, obviously, but if this is implemented over the objection of the community, the Foundation would effectively be signalling intent for Wikipedia to become one more publication that has fired their editors, in part or in whole, in order to put out AI slop.

> It sends the wrong signal to the users (namely that Wikipedia became enshittified).

> Let's keep Wikipedia AI-free – that's a strength, not a weakness.

> It is generative AI material being presented to a reader under our name, and that is a line we should not cross.

> Don't ruin our reputation by falling victim to the craze for dumbing down everything to the lowest possible attention span - and then add insult to injury by doing it with generative AI.

> LLMs are nothing but unethical, mindless plagiarism-machines built with the stolen work of actual humans. Keep this techbro bubble out of Wikipedia.

> Every page has a disclaimer saying that our content is not guaranteed to be valid and so an assertion of validity would be sending a mixed message.

> To properly evaluate what an AI writes takes a lot of brainpower. In my opinion it takes the same or more brainpower as just reading the sources/article yourself and writing it without AI.

> LLMs are incapable of producing anything but inaccurate slop. "Hallucination" is just a fancy word to hide the fact their technology is garbage.

LLMs are sold to us as "assistants" or "researchers" capable of "reasoning." In reality, they are pattern-completion algorithms. They see words in your prompt and choose words that would likely go with them based on the paragraphs they've been trained on. Because of this, they generate syntactically correct sentences. The problem is that syntax is not semantics, and syntactically correct sentences can also be nonsense. Out of many ways to arrange words together in a syntactically correct manner, there are only few ways to come up with a sentence that actually makes sense. An AI will stumble into those occasionally, but most sentences it generates will inevitably be nonsensical.

> Even just for the shady way that training data is used without permission (and in completely non-transparent way), LLMs should never be used in Wikipedia.

> I have an oven with a WiFi facility. That, too, is a solution looking for a problem. Apparenlty I can leave my cold, raw food in it all day going quietly rancid, and use my phone to switch it in when I am on my way home, thus poisonimng all who eat the (now cooked) food.

> Perhaps I should thaw the pizza I my WiFi enabled tumble dryer, another solution looking for a problem, and then use the robot vacuum cleaner to transport it to the oven.

> This looks a bit like starting with the desire to integrate AI, and then working backward to find a place to fit it in.
