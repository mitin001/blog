# [Wikipedians discuss simple summaries](https://en.m.wikipedia.org/wiki/Wikipedia:Village_pump_(technical)/Archive_221#h-Simple_summaries:_editor_survey_and_2-week_mobile_study-20250602182000)

The Google news feed on my Android phone recommended a news story telling me that there's been an anti-AI protest at Wikipedia. I went to check out the heated discussion that ensued and came back with answers to seven questions about AI and Wikipedia.

1. TOC
{::toc}

## How do Wikipedians feel about the Wikimedia Foundation?

The Wikimedia Foundation is the non-profit company that hosts Wikipedia, and Wikipedians have major reservations about trusting this company. The WMF's push to cram AI into Wikipedia shows it misunderstands the community and the project's mission. Loud disagreements broke out between the community and WMF over the foundation's AI push.

> LLMs are nothing but unethical, mindless plagiarism-machines built with the stolen work of actual humans. Keep this techbro bubble out of Wikipedia.

A senior Wikipedia editor even predicted that this is what will finally kill Wikipedia.

> Introducing AI summaries would probably lead to a fork and an exodus. I would honestly be shocked if AI is not the final straw in the relationship between the WMF and the community.

The WMF majorly overstepped the boundary that should exist between the foundation and Wikipedia.

> The foundation cannot dictate content.

Putting AI-generated content into Wikipedia means handing control over Wikipedia's content to the company that maintains the AI models (and presumably patches those models in ways that are beneficial to them). The WMF's idea to make generated content more prominent than written content adds insult to injury.

> They are proposing giving the most important screen real estate we have (the WP:LEAD) of every article to a for-profit company.

Building your own independent AI model (that doesn't serve the interests of some for-profit company) is impossible if you're not wealthy. AI is too expensive to build and requires one to break the law. Only large companies can get away with this.

> Being able to set a system prompt is not control, you'd have to train your own model, which means either copyright violations on a massive scale or training on model exclusively on Wikipedia data, meaning it would be completely inferior to what is available.

Because LLMs are the lawlessly obtained abomination that they are, there is no place for them anywhere near Wikipedia.

> Even just for the shady way that training data is used without permission (and in completely non-transparent way), LLMs should never be used in Wikipedia.

By pushing AI into Wikipedia, the WMF seems to want to accelerate Wikipedia's demise. Had they allowed AI edits deep within articles, it would take some time to bury Wikipedia in AI slop. Instead, they want AI slop to greet Wikipedia readers from the lead of their article of interest.

> Many readers would just glance at the summary instead of reading the article. Since the summary will form the glancers' entire impression of the subject, it needs to be extremely accurate. I suspect it is often not.

Not only is the WMF trying to push AI slop into Wikipedia and feature it more prominently than anything else in any article, but they are also not going through the channels established by the community for proposing changes. They created a survey outside of Wikipedia and asked Wikipedia's guests, not its authors, what they thought about the feature.

> Since the WMF is willing to be this sneaky, I don't think we should feel guilty if we fill in the survey a couple hundred times.

The authors proposed to protest the WMF's betrayal.

> Let's intentionally skew the results! The WMF intentionally skewed it by picking who to show it to; the community should skew the results to tell the WMF to stop trying to put AI in Wikipedia!

The WMF will assert that this is an experiment, and anything can be done in the spirit of experimentation. The representatives of the foundation don't seem to understand that an experiment on the whole world is no longer an experiment. When you foist an experiment on people without them signing up to be the subjects of said experiment, this should be considered a rollout to the general public.

> This cannot be treated as "just" a test. Wikipedia will become the "main character" of social media for a day, as a villain. Thousands of people will hear about the feature, not examine it for themselves, conclude that Wikipedia doesn't care about fact-checking any more, and never learn that the test ended.

This WMF-proposed feature will be Wikipedia's undoing, the community warns. The foundation's motives cannot be any more self-serving.

> Keep AI out of Wikipedia. That is all. WMF staffers looking to pad their resumes with AI-related projects need to be looking for new employers.

Wikipedia is about crowd-sourced knowledge, with a legion of volunteers intent on crafting, through edits and discussions, the best articles possible on any subject they deem notable. AI is the opposite of that. It is instantly generated slop.

> The current climate gives Wikipedia a unique opportunity to be the answer to the AI-ification of everything. Why are we throwing that away?

The crowdsourcing nature of Wikipedia and the painstaking discussions that take place there ensure that everything in the articles is reliable and has a neutral point of view (NPOV), something AI does not do.

> Yes, human editors can introduce reliabilty and NPOV issues. But as a collective mass, it evens out into a beautiful corpus. With Simple Article Summaries, you propose giving one singular editor with known reliabilty and NPOV issues a platform at the very top of any given article.

Wikipedia prides itself on its extensive multilingual support, something AI will surely degrade.

> They want to dump this trash on multiple wikis, despite the fact that everyone who speaks languages other than English and English knows how much worse the experience in another language is, compared to English.

It will also degrade's Wikipedia reputation as a reliable source of high-quality information.

> Any use of AI to generate user-visitor content will torpedo Wikipedia's current reputation as the last bastion of actual facts on the internet.

It will go against Wikipedia's premise, which is that humans have labored over the content presented to the reader, from phrasing to following notability guidelines to fact checking. Once Wikipedia is contaminated with AI, then all of this can be effectively thrown out. Everything AI touches must include a disclaimer urging people to no longer trust the text. Just think of what an AI-tainted Wikipedia will look like.

> Every page has a disclaimer saying that our content is not guaranteed to be valid and so an assertion of validity would be sending a mixed message.

Not to mention that the volunteers that have worked tirelessly to make Wikipedia what it was before AI don't want to be associated with AI.

> It is generative AI material being presented to a reader under our name, and that is a line we should not cross.

Wikipedians also ask the foundation whether its decision-makers realize that the volunteers will leave if that's how they want to fill their volunteer hours.

> You think people are lining up to check the work of an AI model? Especially when summarizing complicated technical topics most people don't even understand?

No one wants to be harnessed to the machine. No one wants to be a reverse-centaur. But the WMF didn't stop to consider this.

> It shows a real lack of understanding about en-wiki's needs and priorities to propose inventing a new task for admins to deal with, let alone one so spirit-killing as AI cleanup.

With the push for AI, the WMF is showing that it is intent on creating unnecessary problems for Wikipedians.

> They're generating a huge and completely unnecessary cleanup problem for volunteers to handle.

The WMF is showing that even a community of volunteers is not safe from the AI takeover.

> We are not paid, obviously, but if this is implemented over the objection of the community, the Foundation would effectively be signalling intent for Wikipedia to become one more publication that has fired their editors, in part or in whole, in order to put out AI slop.

The WMF wants to repurpose the volunteers maintaining Wikipedia to training AI for free.

> Think of the poor multi-billion dollar AI company who now has to pay up to 2 dollars an hour for people to correct the output of their model! They really deserve our help, for free.

The WMF has no right to demand this of the volunteers that have built up Wikipedia from the ground up. And if the foundation turns around and says that the volunteers don't have to check the AI outputs but show them anyway, that's worse. That's leaving it up to the reader to decide what is true or not. That's how society's biases become entrenched into algorithms.

> When other LLMs ask for thumbs up/down feedback, they are assessing whether you liked the response, not whether you double-checked its accuracy. To implement that here will train models to reinforce reader preconceptions, rather than accurately summarize our articles.

Wikipedia should not be the place where AI models are being fed biased decisions in a frictionless way. Wikipedia exists to enable people to do the due diligence to find their way to the truth.

> Don't ruin our reputation by falling victim to the craze for dumbing down everything to the lowest possible attention span - and then add insult to injury by doing it with generative AI.

The WMF began with the desire to push AI into Wikipedia and then thought up ways where this could fit.

> This looks a bit like starting with the desire to integrate AI, and then working backward to find a place to fit it in.

Except, AI doesn't fit into article summaries.

> Every article on Wikipedia has a WP:LEDE which summarizes the article. We do not need another summary to summarize that summary above the summary. This is a solution looking for a problem.

AI doesn't fit anywhere on Wikipedia.

> In a world where Wikipedia is one of the only respected bastions against hallucinated bullshit information by LLMs, our response is to implement them onto the project?

Let's not cram AI into it.

> It sends the wrong signal to the users (namely that Wikipedia became enshittified).

Let's not enshittify Wikipedia. 

> Let's keep Wikipedia AI-free – that's a strength, not a weakness.

## Does AI make someone more efficient?

It depends on what they're doing with it. If they use it to try to get quality information, then absolutely not.

> To properly evaluate what an AI writes takes a lot of brainpower. In my opinion it takes the same or more brainpower as just reading the sources/article yourself and writing it without AI.

## Why is the AI trend still not dead?

It should be, but it serves the interests of too many people to die without first wreaking havoc on our software. Putting AI into a product does not make it better. It is a self-serving act of selfish managers and boneheaded engineers who desperately want to cash in on the AI hype before it's over. When the WMF pushed to put AI summaries into Wikipedia, many in the community felt like the foundation should've talked to them first before putting presumably a lot of work into implementing the AI feature. It was a waste of engineering hours. However, one member of the community doubted that it was a waste of *that* many hours.

> Did a lot of work even go into this? It takes all of 5 minutes to set up the world's 327482th ChatGPT wrapper claiming to be a brand new feature. It feels slapdash and easy for them to throw up and it's such an insult to the community.

The current AI craze is just another way to elevate lazy engineers into positions of power, just because they traded their integrity for giving in to a trend. Wikipedians do not want this for their beloved encyclopedia.

> This would do immediate and irreversible harm to our readers and to our reputation as a decently trustworthy and serious source.

Because everyone has lost their mind in a bloodless pursuit of easy cash from riding the wave of trendy new technology, there a stampede of AI-generated crap just about everywhere you turn.

> Let's not insult our readers' intelligence and join the stampede.

Meanwhile, no user wants AI in the products they find useful without it.

> Haven’t we been getting good press for being a more reliable alternative to AI summaries in search engines? If they’re getting the wrong answers, let’s not copy their homework.

Those who want to read AI-generated crap can find it in AI products. Integrating those into non-AI products does not help the consumer who benefited from those non-AI products.

> AI consumers knowingly interact with is trained on Wikipedia, so they don't need wikipedia.org for that. So the WMF is proposing making a shittier version of something that already exists.

Internet communities are not subject to game theory. They are not competing for the users' dollars. At least, Wikipedia isn't. It doesn't need to give in to passing fads like AI overviews just because others do. Wikipedia and other Internet communities don't need to fear mutually assured destruction.

> It is not like a nuke, we don't need to have it just because others do.

## Has AI turned against humans and started to deliberately kill the Internet by burying it in its slop?

Certainly not, despite all appearances. A conspiracy theorist may notice that AI sees Wikipedia as competition and aims to destroy it by burying it in slop.

> Wikipedia and AI are in the same business (summarizing) and we humans at Wikipedia are better at it than AI. I see little good that can come from mixing in hallucinated AI summaries next to our high quality summaries, when we can just have our high quality summaries by themselves.

AI seduces the people in charge of human initiatives with its capacity to generate infinite content so they let it destroy those initiatives.

> Wikipedia's brand is reliability, traceability of changes and "anyone can fix it". AI is the opposite of these things. Public sentiment (among some corners) is reacting against the AI trend quite harshly.

Maybe AI really will destroy humanity by replacing all our written word with its great scramble. We won't be able to communicate with each other because every mode of communication will be hijacked by the AI scrambler.

## Are LLMs capable of assisting, researching, or reasoning?

No, despite the claims of their marketers. LLMs are sold to us as "assistants" or "researchers" capable of "reasoning." In reality, they are pattern-completion algorithms. They see words in your prompt and choose words that would likely go with them based on the paragraphs they've been trained on. Because of this, they generate syntactically correct sentences. The problem is that syntax is not semantics, and syntactically correct sentences can also be nonsense. Out of many ways to arrange words together in a syntactically correct manner, there are only a few ways to come up with a sentence that actually makes sense. An AI will stumble into those occasionally, but most sentences it generates will inevitably be nonsensical.

> LLMs are incapable of producing anything but inaccurate slop. "Hallucination" is just a fancy word to hide the fact their technology is garbage.

## Are AI companies interested in making the outputs of their products factually correct?

No, they don't cite their sources, and when they do, the sources often don't say what the LLM's output alleges they say. This is in stark contrast with academic literature and previous "disruptors" of academic integrity like Wikipedia.

> Readers come to the site trusting that we can give them all the information they want, while (crucially!) substantiating everything we say with sourcing.

Meanwhile, the AI chatbots are specifically engineered to muddy their responses.

> It is made to give different replies even when asked the same question. I don’t believe the replies should differ, it should give me what it believes to be most likely correct.

Had the AI companies been interested in supplying their users with the most accurate information, their chatbots wouldn't be programmed to give different responses to the same prompts. There would be the best, most accurate way to answer the prompt, and the user would be served that. Instead, we get a random word generator constrained by the rules of syntax, not semantics.

## Why do people create solutions and then look for problems to which their solution could apply?

Because salespeople like the term "integration." It gives them fodder for their sales pitches, even if the integrated features are useless. An AI summary of a Wikipedia article is a mess of randomly generated text filtered through a model trained on stolen copyrighted material to loosely fit together with the terms used in the article, through which process it loses the neutral tone of the article and makes up information not contained in it. No one wants to read it, and Wikipedia already has a summary of every article written by humans, but that didn't stop the WMF from trying to sell this feature to the Wikipedians and the public at large. AI exists to come up with keywords that could improve optical character recognition or audio transcription or Internet search. There's no reason it needs to be used to generate text that's passed off as informative, something it cannot do effectively. Similarly, Internet connectivity comes with risks. An oven doesn't need to be connected to the Internet.

> I have an oven with a WiFi facility. That, too, is a solution looking for a problem. Apparenlty I can leave my cold, raw food in it all day going quietly rancid, and use my phone to switch it in when I am on my way home, thus poisonimng all who eat the (now cooked) food.

Not only is it risky to let your devices accept connections from anyone on the open Internet, it is utterly unnessessary.

> Perhaps I should thaw the pizza I my WiFi enabled tumble dryer, another solution looking for a problem, and then use the robot vacuum cleaner to transport it to the oven.
