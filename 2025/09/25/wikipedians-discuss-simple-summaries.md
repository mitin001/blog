# [Wikipedians discuss simple summaries](https://en.m.wikipedia.org/wiki/Wikipedia:Village_pump_(technical)/Archive_221#h-Simple_summaries:_editor_survey_and_2-week_mobile_study-20250602182000)

The Google news feed on my Android phone recommended a news story telling me that there's been an anti-AI protest at Wikipedia. I went to check out the heated discussion that ensued and came back with answers to six questions about AI and Wikipedia.

1. TOC
{:toc}

## How do Wikipedians feel about the Wikimedia Foundation?

The Wikimedia Foundation is the non-profit company that hosts Wikipedia, and Wikipedians have major reservations about trusting this company. The WMF's push to cram AI into Wikipedia shows it misunderstands the community and the project's mission. Loud disagreements broke out between the community and WMF over the foundation's AI push. A senior Wikipedia editor even predicted that this is what will finally kill Wikipedia.

> Introducing AI summaries would probably lead to a fork and an exodus. I would honestly be shocked if AI is not the final straw in the relationship between the WMF and the community.

The WMF majorly overstepped the boundary that should exist between the foundation and Wikipedia.

> The foundation cannot dictate content.

Putting AI-generated content into Wikipedia means handing control over Wikipedia's content to the company that maintains the AI models (and presumably patches those models in ways that are beneficial to them). The WMF's idea to make generated content more prominent than written content adds insult to injury.

> They are proposing giving the most important screen real estate we have (the WP:LEAD) of every article to a for-profit company.

Building your own independent AI model (that doesn't serve the interests of some for-profit company) is impossible if you're not wealthy. AI is too expensive to build and requires one to break the law. Only large companies can get away with this.

> Being able to set a system prompt is not control, you'd have to train your own model, which means either copyright violations on a massive scale or training on model exclusively on Wikipedia data, meaning it would be completely inferior to what is available.

By pushing AI into Wikipedia, the WMF seems to want to accelerate Wikipedia's demise. Had they allowed AI edits deep within articles, it would take some time to bury Wikipedia in AI slop. Instead, they want AI slop to greet Wikipedia readers from the lead of their article of interest.

> Many readers would just glance at the summary instead of reading the article. Since the summary will form the glancers' entire impression of the subject, it needs to be extremely accurate. I suspect it is often not.

Not only is the WMF trying to push AI slop into Wikipedia and feature it more prominently than anything else in any article, but they are also not going through the channels established by the community for proposing changes. They created a survey outside of Wikipedia and asked Wikipedia's guests, not its authors, what they thought about the feature.

> Since the WMF is willing to be this sneaky, I don't think we should feel guilty if we fill in the survey a couple hundred times.

The authors proposed to protest the WMF's betrayal.

> Let's intentionally skew the results! The WMF intentionally skewed it by picking who to show it to; the community should skew the results to tell the WMF to stop trying to put AI in Wikipedia!

This WMF-proposed feature will be Wikipedia's undoing, the community warns. The foundation's motives cannot be any more self-serving.

> Keep AI out of Wikipedia. That is all. WMF staffers looking to pad their resumes with AI-related projects need to be looking for new employers.

Wikipedia is about crowd-sourced knowledge, with a legion of volunteers intent on crafting, through edits and discussions, the best articles possible on any subject they deem notable. AI is the opposite of that. It is instantly generated slop.

> The current climate gives Wikipedia a unique opportunity to be the answer to the AI-ification of everything. Why are we throwing that away?

The crowdsourcing nature of Wikipedia and the painstaking discussions that take place there ensure that everything in the articles is reliable and has a neutral point of view (NPOV), something AI does not do.

> Yes, human editors can introduce reliabilty and NPOV issues. But as a collective mass, it evens out into a beautiful corpus. With Simple Article Summaries, you propose giving one singular editor with known reliabilty and NPOV issues a platform at the very top of any given article.

Wikipedia prides itself on its extensive multilingual support, something AI will surely degrade.

> They want to dump this trash on multiple wikis, despite the fact that everyone who speaks languages other than English and English knows how much worse the experience in another language is, compared to English.

Wikipedians also ask the foundation whether its decision-makers realize that the volunteers will leave if that's how they want to fill their volunteer hours.

> You think people are lining up to check the work of an AI model? Especially when summarizing complicated technical topics most people don't even understand?

With the push for AI, the WMF is showing that it is intent on creating unnecessary problems for Wikipedians.

> They're generating a huge and completely unnecessary cleanup problem for volunteers to handle.

> Every article on Wikipedia has a WP:LEDE which summarizes the article. We do not need another summary to summarize that summary above the summary. This is a solution looking for a problem.

> Any use of AI to generate user-visitor content will torpedo Wikipedia's current reputation as the last bastion of actual facts on the internet.

> This cannot be treated as "just" a test. Wikipedia will become the "main character" of social media for a day, as a villain. Thousands of people will hear about the feature, not examine it for themselves, conclude that Wikipedia doesn't care about fact-checking any more, and never learn that the test ended.

> Think of the poor multi-billion dollar AI company who now has to pay up to 2 dollars an hour for people to correct the output of their model! They really deserve our help, for free.

> It shows a real lack of understanding about en-wiki's needs and priorities to propose inventing a new task for admins to deal with, let alone one so spirit-killing as AI cleanup.

> When other LLMs ask for thumbs up/down feedback, they are assessing whether you liked the response, not whether you double-checked its accuracy. To implement that here will train models to reinforce reader preconceptions, rather than accurately summarize our articles.

> In a world where Wikipedia is one of the only respected bastions against hallucinated bullshit information by LLMs, our response is to implement them onto the project?

> We are not paid, obviously, but if this is implemented over the objection of the community, the Foundation would effectively be signalling intent for Wikipedia to become one more publication that has fired their editors, in part or in whole, in order to put out AI slop.

> It sends the wrong signal to the users (namely that Wikipedia became enshittified).

> Let's keep Wikipedia AI-free – that's a strength, not a weakness.

> It is generative AI material being presented to a reader under our name, and that is a line we should not cross.

> Don't ruin our reputation by falling victim to the craze for dumbing down everything to the lowest possible attention span - and then add insult to injury by doing it with generative AI.

> Every page has a disclaimer saying that our content is not guaranteed to be valid and so an assertion of validity would be sending a mixed message.

> To properly evaluate what an AI writes takes a lot of brainpower. In my opinion it takes the same or more brainpower as just reading the sources/article yourself and writing it without AI.

> Even just for the shady way that training data is used without permission (and in completely non-transparent way), LLMs should never be used in Wikipedia.

> This looks a bit like starting with the desire to integrate AI, and then working backward to find a place to fit it in.

> LLMs are nothing but unethical, mindless plagiarism-machines built with the stolen work of actual humans. Keep this techbro bubble out of Wikipedia.

## Why is the AI trend still not dead?

It should be, but it serves the interests of too many people to die without first wreaking havoc on our software. Putting AI into a product does not make it better. It is a self-serving act of selfish managers and boneheaded engineers who desperately want to cash in on the AI hype before it's over. When the WMF pushed to put AI summaries into Wikipedia, many in the community felt like the foundation should've talked to them first before putting presumably a lot of work into implementing the AI feature. It was a waste of engineering hours. However, one member of the community doubted that it was a waste of *that* many hours.

> Did a lot of work even go into this? It takes all of 5 minutes to set up the world's 327482th ChatGPT wrapper claiming to be a brand new feature. It feels slapdash and easy for them to throw up and it's such an insult to the community.

The current AI craze is just another way to elevate lazy engineers into positions of power, just because they traded their integrity for giving in to a trend. Wikipedians do not want this for their beloved encyclopedia.

> This would do immediate and irreversible harm to our readers and to our reputation as a decently trustworthy and serious source.

Because everyone has lost their mind in a bloodless pursuit of easy cash from riding the wave of trendy new technology, there a stampede of AI-generated crap just about everywhere you turn.

> Let's not insult our readers' intelligence and join the stampede.

Meanwhile, no user wants AI in the products they find useful without it.

> Haven’t we been getting good press for being a more reliable alternative to AI summaries in search engines? If they’re getting the wrong answers, let’s not copy their homework.

Those who want to read AI-generated crap can find it in AI products. Integrating those into non-AI products does not help the consumer who benefited from those non-AI products.

> AI consumers knowingly interact with is trained on Wikipedia, so they don't need wikipedia.org for that. So the WMF is proposing making a shittier version of something that already exists.

Internet communities are not subject to game theory. They are not competing for the users' dollars. At least, Wikipedia isn't. It doesn't need to give in to passing fads like AI overviews just because others do. Wikipedia and other Internet communities don't need to fear mutually assured destruction.

> It is not like a nuke, we don't need to have it just because others do.

## Has AI turned against humans and started to deliberately kill the Internet by burying it in its slop?

Certainly not, despite all appearances. A conspiracy theorist may notice that AI sees Wikipedia as competition and aims to destroy it by burying it in slop.

> Wikipedia and AI are in the same business (summarizing) and we humans at Wikipedia are better at it than AI. I see little good that can come from mixing in hallucinated AI summaries next to our high quality summaries, when we can just have our high quality summaries by themselves.

AI seduces the people in charge of human initiatives with its capacity to generate infinite content so they let it destroy those initiatives.

> Wikipedia's brand is reliability, traceability of changes and "anyone can fix it". AI is the opposite of these things. Public sentiment (among some corners) is reacting against the AI trend quite harshly.

Maybe AI really will destroy humanity by replacing all our written word with its great scramble. We won't be able to communicate with each other because every mode of communication will be hijacked by the AI scrambler.

## Are LLMs capable of assisting, researching, or reasoning?

No, despite the claims of their marketers. LLMs are sold to us as "assistants" or "researchers" capable of "reasoning." In reality, they are pattern-completion algorithms. They see words in your prompt and choose words that would likely go with them based on the paragraphs they've been trained on. Because of this, they generate syntactically correct sentences. The problem is that syntax is not semantics, and syntactically correct sentences can also be nonsense. Out of many ways to arrange words together in a syntactically correct manner, there are only a few ways to come up with a sentence that actually makes sense. An AI will stumble into those occasionally, but most sentences it generates will inevitably be nonsensical.

> LLMs are incapable of producing anything but inaccurate slop. "Hallucination" is just a fancy word to hide the fact their technology is garbage.

## Are AI companies interested in making the outputs of their products factually correct?

No, they don't cite their sources, and when they do, the sources often don't say what the LLM's output alleges they say. This is in stark contrast with academic literature and previous "disruptors" of academic integrity like Wikipedia.

> Readers come to the site trusting that we can give them all the information they want, while (crucially!) substantiating everything we say with sourcing.

Meanwhile, the AI chatbots are specifically engineered to muddy their responses.

> It is made to give different replies even when asked the same question. I don’t believe the replies should differ, it should give me what it believes to be most likely correct.

Had the AI companies been interested in supplying their users with the most accurate information, their chatbots wouldn't be programmed to give different responses to the same prompts. There would be the best, most accurate way to answer the prompt, and the user would be served that. Instead, we get a random word generator constrained by the rules of syntax, not semantics.

## Why do people create solutions and then look for problems to which their solution could apply?

> I have an oven with a WiFi facility. That, too, is a solution looking for a problem. Apparenlty I can leave my cold, raw food in it all day going quietly rancid, and use my phone to switch it in when I am on my way home, thus poisonimng all who eat the (now cooked) food.

> Perhaps I should thaw the pizza I my WiFi enabled tumble dryer, another solution looking for a problem, and then use the robot vacuum cleaner to transport it to the oven.
