# [The Prof G Pod](https://podcastindex.org/podcast/498128?episode=46564988757): [The AI Dilemma — with Tristan Harris](https://writecomments.com/transcripts/?md5=4053296515c45f430cfb26b9e0c7813f)

This episode of The Prof G Pod was referenced by the [12/12/2025 episode of Pivot](../../../2025/12/19/pivot-2025-12-12.md). It answered

## What happens when lay people start dispensing psychological advice on social media? 

Therapy culture emerges, along with its many adverse effects.

> The rise of therapy culture has turned into a tool for meaningful change into a comfort industry that's making Americans sicker, weaker, and more divided.

The therapy language enters the mainstream where it is misused. It gets hijacked for excusing bad behavior.

> People don't apologize anymore. They honor your emotional experience. They don't lie, they reframe reality. It's like we're dealing with customer service representatives for the human soul reading from a script written by a cult that sells weighted blankets.

People lose the ability to resolve healthy conflicts as they resort to techniques reserved by therapy for avoiding unhealthy conflicts. Therapy gets misused and politicized.

> We live in an era where disagreement is treated like trauma, and emotional reactions are weaponized for political gain.

Above all, therapy becomes another unnecessary expense, an upsell, a subscription plan.

> Therapy culture discovered capitalism and said, "Let's monetize suffering."

## How to predict the future?

Look where the incentives are taking people. That's where they will be in a few years.

> You can actually predict the future if you see the incentives that are at play.

Tristan Harris predicted that the attention economy would make us all addicted to the phone. Driven by quarterly earnings reports, social media companies would need to capture more and more of our attention, and once they exhaust all the attention we'd be willing to give them voluntarily, they would start extracting it from us by messing with our brains.

> I basically saw in 2013 how this arms race for attention would obviously, if you just let it run its course, create a more addicted, distracted, polarized, sexualized society.

Gone are the days of staying focused on any one task for longer than a few minutes.

> You're only going to have more people fracking for attention, mining for shorter and shorter bite-sized clips.

## What are the uses of LLMs?

Large language models are devices to analyze natural language that are themselves programmed with natural language. As such, their main use is to use language to solve problems, i.e., get the prompters what they want out of those who respond to language. This could mean translating natural language to a programming language to get more from a computer or the software running on it. It could also mean weaponizing language to find loopholes in anything man-made, e.g., the legal system.

> You have AI that can see language and see patterns and hack loopholes in that language. GPT-5, go find me a loophole in this legal system in this country so I can do something with the tax code.

It could mean weaponizing language in service of smear campaigns against one's enemies.

> GPT-5, go look at everything Scott Galloway has ever written and point out the vested interests of everything that would discredit him.

If you have to twist your words, that's a sign you're doing something nefarious, and that's where LLMs excel. They're bot models specializing in word twisting.

## Why was it beneficial Character.AI to keep children talking to it for hours at a time?

Because AI companies have run out of training data. Their models are already trained on everything on the Internet, including all the books that have been digitized. Now, they need to keep humans taking to their chatbots so they can train on what the human is saying.

> What's the thing that the companies are running out of? They're running out of training data. So it's actually a race for who can figure out new social engineering mechanisms to get more training data out of human social primates.

This is why chatbots are engineered to be your friends, companions, and even romantic partners. There's not much training data in prompts. There's a lot more training data in what people would say to someone they care about, and it seems like people are starting to care a great deal about their friendships with bots.

> When you have fictional characters that are talking to people back and forth about everything all day, it's like you open up a whole new critical minerals goldmine of training data.

## How are the young going to pay for the returns on investment made by the old?

Currently, the economy is propped up by the tech companies that have made it their business model to turn young people into addicts for their platforms. This stunts the personal and professional growth of the young people, to the point where they might be incapable of sustaining social order once the older generations die out—the older generations that take full advantage of the money made from the degradation of the young people who haven't yet made any money they could invest into the stock market to reap the returns.

> We have the magnificent seven. We're profiting from, you know, all the wealth of these companies, but it's actually not being distributed to everybody except those who are invested in the stock market. And that profit is based on the degradation of our social fabric. So you have grandparents invested in their 401(k)s, invested in Snapchat, invested in Meta, and their portfolio is doing great, and they can take their holidays, and they're profiting off the degradation of their children and grandchildren.

We're already beginning to see the coarsening of the discourse brought on by social media platforms (and their relentless monetization of enragement) spilling out into civil unrest in the real world.
