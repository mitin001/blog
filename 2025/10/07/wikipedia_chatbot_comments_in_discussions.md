# [Chatbot comments in discussions](https://en.m.wikipedia.org/wiki/Wikipedia:Village_pump_(policy)/Archive_199#LLM/chatbot_comments_in_discussions)

This discussion was referenced by [Wikipedia's essay on large language models](../../../2025/09/26/wikipedia_large_language_models.md). It answered several questions I had about generative AI, how it differs from semantic translation software, why the use of the former should be proscribed in discussions, why such decisions are not discriminatory, and so on.

## Is prohibiting LLM use an act of discrimination against people who are not proficient English speakers?

> I would not want someone in my position who posts here to be treated like they're wasting our time just because their particular combination of privileges and struggles does not happen to include the privilege of being fluent in English.

And yet, prohibiting LLMs is still not discriminatory against such people. They can use translators rather than LLMs. Translators and LLMs have different objectives. The job of a translator is to remain faithful to the text. They convert text from one language to another, with only a small, manageable portion of it getting lost in translation. An LLM, on the other hand, can significantly stray from the thoughts expressed in the original language.

> Some translation tools have LLM assistance, but the whole point of generative models is to create text far beyond what is found in the user's input.

Generative AI generates text, loosely constrained but the prompt. That isn't the same as translating it. AI outputs must always be verified, and if the person using it can't do so because they don't speak the language to which they're translating their original post.

> Text written by an LLM is not text written by a person. We cannot guarantee it is what the person actually means, especially as it is a tool often used by those with less English proficiency, which means perhaps they cannot evaluate the text themselves.

One could argue that a beginner language learner who may not yet have the ability to formulate sentences in the new language can already be proficient in judging the sentences generated by others as to whether they communicate what the person intended to communicate. Such people could be conceivably entrusted with using an LLM to write for them.

> Reading comprehension skills and writing skills in foreign languages are very frequently not at the same level, it is extremely plausible that someone will be able to understand whether the AI output is what they want to express without having been able to write it themselves directly.

However, the caveats here are too significant. The assumption of a wide gap between writing skills and reading comprehension skills aren't always justified, and an LLM can easily trick even a native speaker to accept its nonsense as fact. LLMs lie confidently. They exploit the human vulnerabilities to anthropomorphize machines and mistake confidence for truthfulness. So, beginner language learners should stick to translators over LLMs.

> ESL editors who use LLMs to make themselves sound like Brad Anderson in middle management should stop doing that because it actually gets in the way of clear communication.

## How are LLMs used in translation software?

Fundamentally different from how LLMs are used in chatbots. When you ask a chatbot to translate a corpus of text, it can stray from it to the point of injecting unnecessary and false statements into it. Translation software, even if it uses similar underlying technology, does not do that. Its objective is to find the closest translation of the text, not to generate new text.

> LLM translation tools don't actually create meaningful content where there wasn't any before; the generative AI aspect is only in the use of their vast training data to characterize the semantic context of your input in the form of mathematical relationships between tokens in an embedding space, and then match it with the collection of tokens most closely resembling it in the other language. There is, definitionally, a high level of creative constraint in what the translation output is since semantic preservation is required, something that is not true for text generation.

## Why write a response yourself when you can just prompt an LLM to write it for you?

LLM use is a signifier of incompetence.

> If you can't make a coherent argument on your own, you are not competent to be participating in the discussion.

Spamming a discussion with a wall of generated text is a step toward burying it in AI slop. This can kill the discussion.

> The majority of AI-generated comments are somewhere between "pointless" and "disruptive".

Burying the Internet in AI slop is what might eventually kill the Internet. An LLM by its nature will write plausible-sounding sentences that are wrong. Checking AI output often takes more work than writing it yourself. If you pass the output off as your own without checking it, your credibility will suffer.

> Someone posting "here's what ChatGPT has to say on the subject" can waste a lot of other editors' time if they feel obligated to explain why ChatGPT is wrong again.

Some may argue that their writing skills are so poor that their credibility will suffer worse without using an LLM to tweak it.

> Say "I" am a person unconfident in my writing. I imagine that when I post my raw language, I embarrass myself, and my credibility vanishes.

However, using an LLM is a signal to your community that you may be arguing in bad faith and have an uncooperative attitude. It's far worse to one's credibility than one's own poorly written sentences. Having an LLM write for you is disrespectful toward the people with which you're supposed to be communicating. It demonstrates a lack of sincerity.

> If you are too lazy to engage in the discussion yourself, why should we engage with you?

Using an LLM is disrespectful because you're wasting people's time with no effort on your part.

> It's disrespectful for an editor to ask someone to spend their time reading a comment if they couldn't be bothered to spend any time writing it.

When you post LLM-generated text onto a discussion board, you're being rude to the members of that discussion.

> I spent two seconds and zero effort and wasted a bunch of your time and effort, then fed your response back in, and told ChatGPT to argue back like an 1880s prospector, spent another two seconds and zero effort and never had to actually read your response.

When posting LLM-generated text, you're putting someone in a position where they must engage with something not meant to be engaged with.

> To spend valuable time arguing with a machine that doesn't actually comprehend what it's arguing.

Even worse, you're pretending to have written something you haven't. You're compelling others to engage with what they think is a human being but in reality is a bot.

> Personal effort is buy-in in a discussion among equals. If your personal effort in a discussion is an LLM prompt, no editor should be expected to engage with you.

It's a rotten thing to do to promise another human that you've written the text you're giving them only for them to discover that it's been generated by a machine.

> If a human didn't write it, humans shouldn't spend time reading it.

Passing LLM-generated text as your writing is a bait-and-switch. Fool me once, shame on me, but you won't fool me twice if I block you.

> Making a computer generate believable gibberish for you is a waste of time, and tricking someone else into reading it should be a blockable offense.

When asked whether one could use ChatGPT to contribute to a discussion on Wikipedia, an experienced editor explained that any discussion could do without the contributions of the people who resort to using CharGPT.

> An editor who has an argument should not use ChatGPT to present it in an unnecessarily verbose manner, and an editor who doesn't have one should not participate in discussion.

Another Wikipedian is even more adamantly opposed to LLMs.

> I genuinely cannot comprehend how some people could see how AI is decimating the internet through spam, bots and disinformation and still think for even one second that we should open the door to it.

Even when you give an LLM your thoughts on the prompt asking to make them more clear to the readers, the LLM's design makes it prone to misrepresenting them and injecting statements made from a non-neutral point of view and those that masquerade as analysis or synthesis of information (but in reality just word salad made of what people frequently say when they analyze or synthesize information, as those statements appear the LLM's training data).

> The comment was generated in a way (whether it's using a program or ghost writer) such that it fails to express actual thoughts by the editor.

LLMs use is closely associated with disruptive activities like spamming and sealioning.

> An AI can pump out text far faster than a human and that can drown out or wear down the opposition if nothing else.

You do not want to be associated with these if you don't want to soil your reputation.

> LLMs empower bad actors, time wasters and those with competence issues, with very little to offer.

LLMs are not reliable sources of content. They are, however, reliable at generating disruptive content that wastes everyone's time.

> Editors cannot be expected to wear themselves out in engaging at length with the results of someone's one-line prompt to an LLM. Where LLM use in articles is destructive of content and source reliability, in discussions it is destructive of editor good faith.

## How to tell if something was written by an LLM?

> LLM comments are usually glaringly obvious (section headers within the comment, imprecise puffery).

Look for empty generalizations, bullet points, and subheadings.

> LLMs tend to use more than one subheading to reiterate points.

Look for excessive repetition among the subheadings.

> They write like a middle schooler that just learned how to make an essay outline before writing.

Look for the completely unnecessary conclusion paragraph.

> In conclusion, they also tend to have a conclusion paragraph for the same reason they use subheadings.

## When is it okay to pass LLM output as your own writing?

When no one can tell that it originated from an LLM.

> If LLM-ness is not glaringly obvious, it is not a problem, and we should not be going after editors for their writing style or because some tool says they look like a bot.

Typically, this requires a lot of tweaking if the prompt and/or response, amounting to more work, so why even involve an LLM in the first place? Perhaps, it helps you brainstorm, is your springboard, and keeps you more engaged in the writing. Whatever your reasons, do your best not to turn in AI slop if you must resort to using an LLM.

> The nuance required here in part is the difference between someone using any automated tool for assistance, versus true bot-like behavior.

Just don't become a bot.

## Is it okay to plagiarize from a chatbot?

No, even if you could argue that this is a victimless crime (since a chatbot cannot be hurt by you stealing from it), the actual victim of such an act of plagiarism is you, and it's perpetrated by the chatbot, against _you_. You got seduced by the chatbot into discarding your own thoughts and replacing them with its output. 

## Is it too draconian of a policy to ban anyone posts LLM-generated output on the spot? 

No. It takes virtually no time to bury a discussion board in AI slop, so users who take a step toward this (by posting even a single LLM-generated wall of text) should be stopped before they do too much damage. An LLM is an instant spam generator. Any discussion should do all it can to protect itself from spammers.

> WikiProject AI Cleanup, are they keeping up with how fast people type a prompt and click a button?

AI can immediately render any discussion unreadable. It can generate infinite text, with which it can bury everything that was there before.

> The issue is noise swamping signal.

Head on to Twitter to see what becomes of a website when it gets overrun by AI disruption bots.

> Assumption of good faith is not suicide pact.

Writing skills are required of everyone who wants to participate in a wiki or any sort if a written discussion.

> Lacking those skills to a serious enough degree makes one not competent to work on a collaborative encyclopedia project, and one will eventually be ejected after causing enough disruption. Something we don't need is LLMs masking for a while that someone has this kind of competence failure; it will just drag out the inevitable. By way of analogy: if I were a kleptomaniac and just unable to stop myself from shoplifting, it would not be okay for me to use a device that scrambled stores' surveillance camera footage to make it more likely for me to get away with more shoplifting (and there would certainly be no obligation on the part of the store owner to turn their cameras off, or to take no action if they catch me stealing, just because I tell them I'm a kleptomaniac and my unconstructive behavior isn't something I can manage.

LLM use is a tell that the person is lacking writing skills, has nothing meaningful to say, and/or is a troll. The sooner such a user is banned from the discussion, the less effort good-faith users must put into restoring the project. Because AI can instantly do ruinous damage to a discussion, the decision to ban users who attempt to sneak LLM-generated nonsense into it should be just as swift.

> We mere humans cannot keep up with LLM bots.

## Is an LLM a random word generator? 

More or less, except it's words that go through a complicated filter so that they are syntactically correct and bear strong relevance to your prompt.

> If a monkey types up Othello, is the play suddenly worth( )less? An LLM is as if the monkey were not selecting words at random, but rather choosing what to type based on contextualized tokens.

The LLM filter is so complicated that even identical prompts yield differently worded responses every time. And yet, because it still just simply stitches words together, not every combination of words is going to yield a sentence that is true. An LLM is inadvertently but inevitably a liar.

> The machine for generating endless quantities of misleading text.

## What to say to someone who's turning in LLM-generated text for work?

Something along the lines of, "If you need ChatGPT to do this work for you, then maybe you should go find something else to do, something that fits your skills and abilities." Modeled after a response from a Wikipedian to all those who wish to use an LLM to post to Wikipedia.

> If you need an LLM to give what you think is meaningful input into a decision-making process on Wikipedia (much less to generate mainspace content for the public), then you need to go find something else to do, something that fits your skills and abilities.

## What will the AI apocalypse look like? 

The trendy technology will be shoved into every aspect of life without anyone realizing that this technology is not ready. And once it's too enmeshed into every activity of daily living, once it's too late to take it out, humans will finally realize that this technology is crap. There's no way to make it do certain things, and it's unpredictable what it will and will not do. You will ask it to lower the temperature of the water in your bath tub, and it will compliment your persistence, and then it won't do it. There will be no pleading with it. AI is a black box, and if you inadvertently poison its context or it suddenly enters a failure loop, there's no telling how to backtrack from it. Meanwhile, overconfident tech bros will put so much stake into this technology, they won't even put a failsafe into what it's connected to.

> If the AI can (with trivial effort) be put into a crazy failure-looping state where it knows it's doing or about to do something wrong but insists on doing it anyway – i.e. during its take-an-action phase it completely loses connection to reality, even it's internal reality much less external input telling it "no!" – then we are in grave danger. This is essentially the mental state of a psychopath.

Another possibility is a complete exclusion of humans from the Internet as it gets overrun by bots and swiftly turned into grey goo.

> We can't have a Wikipedia of LLMs talking to LLMs on how to make LLM content for LLM articles.

## What is perfectionism? 

It's the enemy of the good. It's endlessly deliberating what the perfect solution might look like and, in the process, passing by the good solutions. The long discussion over the perfect policy proscribing AI use in discussions on Wikipedia was an exercise in perfectionism until one of the Wikipedians called it out for what it was and explained that a good but imperfect policy was a significant improvement over what they had and that it would be no problem to improve it further once it was implemented.

> It's a step in the right direction. If it were implemented this way and some uncommon issue arose with that implementation, then we'd tweak it to address that micro-problem. We must not continue to avoid addressing the macro-problem just because someone can imagine edge cases that might not work out ideally.
